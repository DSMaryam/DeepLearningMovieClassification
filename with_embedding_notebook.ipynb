{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path \n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing import sequence\n",
    "import os \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding, Reshape\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C s'appelle Windows\n",
      " Le num‚ro de s‚rie du volume est 460B-0FC5\n",
      "\n",
      " R‚pertoire de C:\\Users\\Maryam\\Documents\n",
      "\n",
      "15/05/2020  21:20    <DIR>          .\n",
      "15/05/2020  21:20    <DIR>          ..\n",
      "15/05/2020  21:16    <DIR>          .ipynb_checkpoints\n",
      "30/04/2020  15:45    <DIR>          000A NON WINDOWS\n",
      "04/05/2020  12:20    <DIR>          aclImdb\n",
      "05/02/2019  20:07    <DIR>          Adobe\n",
      "12/03/2019  10:59    <DIR>          Arduino\n",
      "09/12/2018  22:39    <DIR>          Audacity\n",
      "30/04/2020  15:45    <DIR>          Autre\n",
      "07/10/2019  11:08    <DIR>          Blocs-notes OneNote\n",
      "31/10/2018  13:25           230ÿ759 ContractEmploymentAgreement_ElMehdiCherradi_signed.pdf\n",
      "23/03/2020  10:48    <DIR>          Downloads\n",
      "04/04/2020  21:21    <DIR>          enigme\n",
      "04/03/2019  14:47    <DIR>          LTspiceXVII\n",
      "13/05/2020  13:52    <DIR>          MATLAB\n",
      "01/11/2019  13:31    <DIR>          Matlab fichiers\n",
      "04/10/2018  11:14    <DIR>          ModŠles Office personnalis‚s\n",
      "14/05/2020  12:00       265ÿ677ÿ269 out5.csv\n",
      "15/05/2020  21:21        58ÿ721ÿ250 out7.csv\n",
      "27/09/2019  15:39    <DIR>          PDF Creator\n",
      "18/11/2019  13:03    <DIR>          Sans actualisation disque dur\n",
      "15/05/2020  15:53        18ÿ453ÿ475 titles.csv\n",
      "15/05/2020  21:20             9ÿ019 Untitled.ipynb\n",
      "15/05/2020  16:01             4ÿ657 webscraping.py\n",
      "15/05/2020  21:03             4ÿ151 with_embedding.py\n",
      "04/05/2020  12:03    <DIR>          Zoom\n",
      "               7 fichier(s)      343ÿ100ÿ580 octets\n",
      "              19 R‚p(s)  36ÿ400ÿ594ÿ944 octets libres\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the data...\n",
      "data loaded, processing data...\n",
      "length of one hot enconding   10\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (169996, 100)\n",
      "x_test shape: (20000, 100)\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "maxlen = 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ytrain=[]\n",
    "trainx=[]\n",
    "Ytest=[]\n",
    "testx=[]\n",
    "print(\"loading the data...\")\n",
    "with open('out5.csv',encoding=\"utf8\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        if csv_reader.line_num < 170001:\n",
    "            Ytrain.append(int(row[0].split('/')[0]))\n",
    "            trainx.append(row[0].split('\\t')[1])\n",
    "        elif csv_reader.line_num < 190001:\n",
    "            Ytest.append(int(row[0].split('/')[0]))\n",
    "            testx.append(row[0].split('\\t')[1]) \n",
    "        else:\n",
    "            break    \n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "#194869\n",
    "\n",
    "\n",
    "print(\"data loaded, processing data...\")\n",
    "\n",
    "\n",
    "# create the tokenizer\n",
    "tok= Tokenizer(num_words=max_features, lower=True,filters='\"#$%&()*+-/:<=>@[\\\\]^_`{|}~\\t\\n',split=' ', char_level=False, oov_token=None, document_count=0)\n",
    "# fit the tokenizer on the documents\n",
    "tok.fit_on_texts(trainx)\n",
    "# summarize what was learned\n",
    "#print(train.word_counts)\n",
    "#print(train.document_count)\n",
    "#print(train.word_index)\n",
    "#print(train.word_docs)\n",
    "# integer encode documents\n",
    "Xtrain = tok.texts_to_sequences(trainx)\n",
    "\n",
    "Xtest = tok.texts_to_sequences(testx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Xtrain, y_train = shuffle(Xtrain, Ytrain, random_state=0)\n",
    "#Xtest, y_test = shuffle(Xtest, Ytest, random_state=0)\n",
    "\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Ytrain)\n",
    "encoded_Ytrain = encoder.transform(Ytrain)\n",
    "encoded_Ytest = encoder.transform(Ytest)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "y_train = np_utils.to_categorical(encoded_Ytrain)\n",
    "y_test = np_utils.to_categorical(encoded_Ytest)\n",
    "\n",
    "encoder_length = len(y_train[0])\n",
    "print(\"length of one hot enconding  \", encoder_length)\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(Xtrain, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(Xtest, maxlen=maxlen)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "Ytrain = [x-1 for x in Ytrain]\n",
    "Ytest = [x-1 for x in Ytest]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 20000 is out of bounds for axis 0 with size 20000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-7bfe8bfb2e1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0membedding_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membeddings_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0membedding_vector\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0membedding_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding_vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: index 20000 is out of bounds for axis 0 with size 20000"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.100d.txt',encoding=\"utf8\")\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((max_features, 100))\n",
    "for word, i in tok.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.038194   -0.24487001  0.72812003 ... -0.1459      0.82779998\n",
      "   0.27061999]\n",
      " [-0.071953    0.23127     0.023731   ... -0.71894997  0.86894\n",
      "   0.19539   ]\n",
      " [-0.27085999  0.044006   -0.02026    ... -0.4923      0.63687003\n",
      "   0.23642001]\n",
      " ...\n",
      " [ 0.16046999  0.86734998 -0.085039   ... -0.71705002 -0.15491\n",
      "   0.30443001]\n",
      " [ 0.015885   -0.17775001  1.09720004 ... -0.26109999 -0.34308001\n",
      "   0.44424999]\n",
      " [ 0.10107    -0.90408999  0.89310002 ... -0.71322    -0.08908\n",
      "   0.10901   ]]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 100, 100)          2000000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_10 (Spatia (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 2,081,410\n",
      "Trainable params: 81,410\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 152996 samples, validate on 17000 samples\n",
      "Epoch 1/5\n",
      "152996/152996 [==============================] - 527s 3ms/step - loss: 1.8482 - acc: 0.3266 - val_loss: 1.7442 - val_acc: 0.3452\n",
      "Epoch 2/5\n",
      "152996/152996 [==============================] - 520s 3ms/step - loss: 1.6999 - acc: 0.3718 - val_loss: 1.6598 - val_acc: 0.3808\n",
      "Epoch 3/5\n",
      "152996/152996 [==============================] - 507s 3ms/step - loss: 1.6424 - acc: 0.3920 - val_loss: 1.6223 - val_acc: 0.3938\n",
      "Epoch 4/5\n",
      "152996/152996 [==============================] - 504s 3ms/step - loss: 1.6066 - acc: 0.4047 - val_loss: 1.5888 - val_acc: 0.4021\n",
      "Epoch 5/5\n",
      "152996/152996 [==============================] - 514s 3ms/step - loss: 1.5834 - acc: 0.4105 - val_loss: 1.5796 - val_acc: 0.4064\n",
      "20000/20000 [==============================] - 22s 1ms/step\n",
      "Test score: 1.5612398336410522\n",
      "Test accuracy: 0.40255\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define model\n",
    "# https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "embedding_size = 100\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "e = Embedding(max_features,embedding_size,weights=[embedding_matrix], input_length=maxlen,trainable=False)\n",
    "model.add(e)\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(encoder_length, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "\n",
    "\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 152996 samples, validate on 17000 samples\n",
      "Epoch 1/3\n",
      "  6080/152996 [>.............................] - ETA: 12:55 - loss: 2.0652 - acc: 0.2757 - sparse_categorical_accuracy: 0.2757"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-3e0d8d947976>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mhistory2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "\n",
    "\n",
    "embedding_size = 100\n",
    "\n",
    "def top2_acc(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=2)\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(max_features,embedding_size, input_length=maxlen))\n",
    "model2.add(SpatialDropout1D(0.2))\n",
    "model2.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model2.add(Dense(encoder_length, activation='softmax'))\n",
    "\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 3\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "history2 = model2.fit(x_train, Ytrain, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "\n",
    "\n",
    "\n",
    "score, acc = model2.evaluate(x_test, Ytest, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          2000000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 2,081,410\n",
      "Trainable params: 81,410\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 152996 samples, validate on 17000 samples\n",
      "Epoch 1/10\n",
      "152996/152996 [==============================] - 325s 2ms/step - loss: 1.8882 - acc: 0.3163 - val_loss: 1.7754 - val_acc: 0.3407\n",
      "Epoch 2/10\n",
      "152996/152996 [==============================] - 316s 2ms/step - loss: 1.7519 - acc: 0.3552 - val_loss: 1.7199 - val_acc: 0.3572\n",
      "Epoch 3/10\n",
      "152996/152996 [==============================] - 337s 2ms/step - loss: 1.6971 - acc: 0.3739 - val_loss: 1.6818 - val_acc: 0.3758\n",
      "Epoch 4/10\n",
      "152996/152996 [==============================] - 316s 2ms/step - loss: 1.6642 - acc: 0.3856 - val_loss: 1.6603 - val_acc: 0.3842\n",
      "Epoch 5/10\n",
      "152996/152996 [==============================] - 315s 2ms/step - loss: 1.6434 - acc: 0.3926 - val_loss: 1.6303 - val_acc: 0.3914\n",
      "Epoch 6/10\n",
      "152996/152996 [==============================] - 323s 2ms/step - loss: 1.6271 - acc: 0.3974 - val_loss: 1.6386 - val_acc: 0.3904\n",
      "Epoch 7/10\n",
      "152996/152996 [==============================] - 323s 2ms/step - loss: 1.6138 - acc: 0.4007 - val_loss: 1.6240 - val_acc: 0.3972\n",
      "Epoch 8/10\n",
      "152996/152996 [==============================] - 303s 2ms/step - loss: 1.6046 - acc: 0.4046 - val_loss: 1.6215 - val_acc: 0.3965\n",
      "Epoch 9/10\n",
      "152996/152996 [==============================] - 282s 2ms/step - loss: 1.5981 - acc: 0.4063 - val_loss: 1.6056 - val_acc: 0.3969\n",
      "Epoch 10/10\n",
      "152996/152996 [==============================] - 282s 2ms/step - loss: 1.5921 - acc: 0.4071 - val_loss: 1.6027 - val_acc: 0.4027\n",
      "20000/20000 [==============================] - 11s 559us/step\n",
      "Test score: 1.5882123666763306\n",
      "Test accuracy: 0.39985\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define model\n",
    "# https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "embedding_size = 100\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "e = Embedding(max_features,embedding_size,weights=[embedding_matrix], input_length=maxlen,trainable=False)\n",
    "model.add(e)\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(encoder_length, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "\n",
    "\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 100)          2000000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 2,081,410\n",
      "Trainable params: 81,410\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 152996 samples, validate on 17000 samples\n",
      "Epoch 1/20\n",
      "152996/152996 [==============================] - 324s 2ms/step - loss: 1.8812 - acc: 0.3192 - val_loss: 1.8268 - val_acc: 0.3243\n",
      "Epoch 2/20\n",
      "152996/152996 [==============================] - 329s 2ms/step - loss: 1.7586 - acc: 0.3528 - val_loss: 1.7712 - val_acc: 0.3450\n",
      "Epoch 3/20\n",
      "152996/152996 [==============================] - 350s 2ms/step - loss: 1.7114 - acc: 0.3685 - val_loss: 1.6922 - val_acc: 0.3712\n",
      "Epoch 4/20\n",
      "152996/152996 [==============================] - 315s 2ms/step - loss: 1.6778 - acc: 0.3814 - val_loss: 1.6691 - val_acc: 0.3780\n",
      "Epoch 5/20\n",
      "152996/152996 [==============================] - 303s 2ms/step - loss: 1.6554 - acc: 0.3879 - val_loss: 1.6471 - val_acc: 0.3829\n",
      "Epoch 6/20\n",
      "152996/152996 [==============================] - 311s 2ms/step - loss: 1.6384 - acc: 0.3943 - val_loss: 1.6402 - val_acc: 0.3875\n",
      "Epoch 7/20\n",
      "152996/152996 [==============================] - 301s 2ms/step - loss: 1.6266 - acc: 0.3974 - val_loss: 1.6183 - val_acc: 0.3928\n",
      "Epoch 8/20\n",
      "152996/152996 [==============================] - 322s 2ms/step - loss: 1.6142 - acc: 0.4017 - val_loss: 1.6132 - val_acc: 0.3962\n",
      "Epoch 9/20\n",
      "152996/152996 [==============================] - 344s 2ms/step - loss: 1.6053 - acc: 0.4035 - val_loss: 1.6036 - val_acc: 0.3968\n",
      "Epoch 10/20\n",
      "152996/152996 [==============================] - 339s 2ms/step - loss: 1.5970 - acc: 0.4054 - val_loss: 1.6048 - val_acc: 0.3952\n",
      "Epoch 11/20\n",
      "152996/152996 [==============================] - 338s 2ms/step - loss: 1.5913 - acc: 0.4085 - val_loss: 1.6008 - val_acc: 0.3988\n",
      "Epoch 12/20\n",
      "152996/152996 [==============================] - 331s 2ms/step - loss: 1.5853 - acc: 0.4109 - val_loss: 1.5950 - val_acc: 0.3998\n",
      "Epoch 13/20\n",
      "152996/152996 [==============================] - 330s 2ms/step - loss: 1.5826 - acc: 0.4100 - val_loss: 1.5978 - val_acc: 0.4011\n",
      "Epoch 14/20\n",
      "152996/152996 [==============================] - 322s 2ms/step - loss: 1.5784 - acc: 0.4124 - val_loss: 1.5968 - val_acc: 0.4030\n",
      "Epoch 15/20\n",
      "152996/152996 [==============================] - 558s 4ms/step - loss: 1.5739 - acc: 0.4146 - val_loss: 1.5994 - val_acc: 0.3990\n",
      "20000/20000 [==============================] - 24s 1ms/step\n",
      "Test score: 1.5799105220794678\n",
      "Test accuracy: 0.404\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define model\n",
    "# https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "embedding_size = 100\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "e = Embedding(max_features,embedding_size,weights=[embedding_matrix], input_length=maxlen,trainable=False)\n",
    "model.add(e)\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(encoder_length, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "\n",
    "\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 152996 samples, validate on 17000 samples\n",
      "Epoch 1/2\n",
      "152996/152996 [==============================] - 361s 2ms/step - loss: 0.0803 - val_loss: 0.0776\n",
      "Epoch 2/2\n",
      "152996/152996 [==============================] - 358s 2ms/step - loss: 0.0767 - val_loss: 0.0756\n",
      "20000/20000 [==============================] - 14s 682us/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-1a51b4087af1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test score:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test accuracy:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float64' object is not iterable"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.metrics import top_k_categorical_accuracy \n",
    "\n",
    "embedding_size = 100\n",
    "\n",
    "model = Sequential()\n",
    "e = Embedding(max_features,embedding_size,weights=[embedding_matrix], input_length=maxlen,trainable=False)\n",
    "model.add(e)\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(encoder_length, activation='softmax'))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "epochs = 2\n",
    "batch_size = 64\n",
    "\n",
    "#Ytrain = [x-1 for x in Ytrain]\n",
    "#Ytest = [x-1 for x in Ytest]\n",
    "\n",
    "\n",
    "history2 = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "\n",
    "\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 11s 557us/step\n",
      "Test score: 0.07526018905639649\n"
     ]
    }
   ],
   "source": [
    "\n",
    "score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.14509894 0.03687759 0.03119751 0.01546801 0.02140452 0.02866917\n",
      "  0.05456748 0.09557567 0.12362949 0.44751167]] 10\n"
     ]
    }
   ],
   "source": [
    "new_review = ['Oh my God ! This movie was awesome, just perfect, I could not think of anything better. All the scenes were just so nice to watch']\n",
    "seq = tok.texts_to_sequences(new_review)\n",
    "padded = sequence.pad_sequences(seq, maxlen=maxlen)\n",
    "pred = model.predict(padded)\n",
    "labels = [1,2,3,4,5,6,7,8,9,10]\n",
    "print(pred, labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5646696  0.10991977 0.10340281 0.0578124  0.03570674 0.02500441\n",
      "  0.02539086 0.02639755 0.0181576  0.03353834]] 1\n"
     ]
    }
   ],
   "source": [
    "new_review = ['What a stupid movie. I really did not like it, it was very bad and the director did the worse job of his whole career']\n",
    "seq = tok.texts_to_sequences(new_review)\n",
    "padded = sequence.pad_sequences(seq, maxlen=maxlen)\n",
    "pred = model.predict(padded)\n",
    "labels = [1,2,3,4,5,6,7,8,9,10]\n",
    "print(pred, labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17072645 0.05555867 0.05578766 0.03812353 0.04418208 0.04738629\n",
      "  0.09064323 0.12582459 0.11485077 0.25691676]] 10\n"
     ]
    }
   ],
   "source": [
    "new_review = [\"I am quite mitigated, I don't know what to think of that movie. It was good and bad at the same time. Surprising and predictible.\"]\n",
    "seq = tok.texts_to_sequences(new_review)\n",
    "padded = sequence.pad_sequences(seq, maxlen=maxlen)\n",
    "pred = model.predict(padded)\n",
    "labels = [1,2,3,4,5,6,7,8,9,10]\n",
    "print(pred, labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"with_embedding_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 152996 samples, validate on 17000 samples\n",
      "Epoch 1/2\n",
      "152996/152996 [==============================] - 448s 3ms/step - loss: 0.0755 - val_loss: 0.0735\n",
      "Epoch 2/2\n",
      "152996/152996 [==============================] - 439s 3ms/step - loss: 0.0694 - val_loss: 0.0716\n",
      "20000/20000 [==============================] - 13s 636us/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-bca439be87e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test score:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test accuracy:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float64' object is not iterable"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.metrics import top_k_categorical_accuracy \n",
    "\n",
    "embedding_size = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features,embedding_size, input_length=maxlen))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(encoder_length, activation='softmax'))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "epochs = 2\n",
    "batch_size = 64\n",
    "\n",
    "#Ytrain = [x-1 for x in Ytrain]\n",
    "#Ytest = [x-1 for x in Ytest]\n",
    "\n",
    "\n",
    "history2 = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "\n",
    "\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 11s 564us/step\n",
      "Test score: 0.07158285917043686\n"
     ]
    }
   ],
   "source": [
    "score= model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('without_embedding_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03288252 0.01065903 0.00577785 0.00667291 0.00762039 0.0096976\n",
      "  0.02153268 0.0400192  0.10282385 0.76231396]] 10\n"
     ]
    }
   ],
   "source": [
    "new_review = ['Oh my God ! This movie was awesome, just perfect, I could not think of anything better. All the scenes were just so nice to watch']\n",
    "seq = tok.texts_to_sequences(new_review)\n",
    "padded = sequence.pad_sequences(seq, maxlen=maxlen)\n",
    "pred = model.predict(padded)\n",
    "labels = [1,2,3,4,5,6,7,8,9,10]\n",
    "print(pred, labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6464631  0.12189397 0.08601493 0.03376023 0.02737855 0.02094109\n",
      "  0.01476607 0.01635375 0.00951275 0.02291551]] 1\n"
     ]
    }
   ],
   "source": [
    "new_review = ['What a stupid movie. I really did not like it, it was very bad and the director did the worse job of his whole career']\n",
    "seq = tok.texts_to_sequences(new_review)\n",
    "padded = sequence.pad_sequences(seq, maxlen=maxlen)\n",
    "pred = model.predict(padded)\n",
    "labels = [1,2,3,4,5,6,7,8,9,10]\n",
    "print(pred, labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06163768 0.04243037 0.04357338 0.05787824 0.06657224 0.06494371\n",
      "  0.14852828 0.13370521 0.20629641 0.17443444]] 9\n"
     ]
    }
   ],
   "source": [
    "new_review = [\"I am quite mitigated, I don't know what to think of that movie. It was good and bad at the same time. Surprising and predictible.\"]\n",
    "seq = tok.texts_to_sequences(new_review)\n",
    "padded = sequence.pad_sequences(seq, maxlen=maxlen)\n",
    "pred = model.predict(padded)\n",
    "labels = [1,2,3,4,5,6,7,8,9,10]\n",
    "print(pred, labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 152996 samples, validate on 17000 samples\n",
      "Epoch 1/2\n",
      "152996/152996 [==============================] - 464s 3ms/step - loss: 0.0894 - val_loss: 0.0889\n",
      "Epoch 2/2\n",
      "152996/152996 [==============================] - 476s 3ms/step - loss: 0.0881 - val_loss: 0.0878\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "embedding_size = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features,embedding_size, input_length=maxlen))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(encoder_length, activation='softmax'))\n",
    "\n",
    "model.compile(loss='mse', optimizer='sgd')\n",
    "\n",
    "epochs = 2\n",
    "batch_size = 64\n",
    "\n",
    "#Ytrain = [x-1 for x in Ytrain]\n",
    "#Ytest = [x-1 for x in Ytest]\n",
    "\n",
    "\n",
    "history2 = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 12s 596us/step\n",
      "Test score: 0.08762401130199432\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 152996 samples, validate on 17000 samples\n",
      "Epoch 1/2\n",
      "152996/152996 [==============================] - 18s 118us/step - loss: 1.9246 - acc: 0.2996 - val_loss: 1.9012 - val_acc: 0.3038\n",
      "Epoch 2/2\n",
      "152996/152996 [==============================] - 18s 116us/step - loss: 1.8204 - acc: 0.3320 - val_loss: 1.8851 - val_acc: 0.3119\n",
      "20000/20000 [==============================] - 1s 53us/step\n",
      "Test score: 1.8610599002838135\n",
      "Test accuracy: 0.31305\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "\n",
    "from keras.layers import Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "embedding_size = 100\n",
    "\n",
    "model = Sequential()\n",
    "e = Embedding(max_features,embedding_size,weights=[embedding_matrix], input_length=maxlen,trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(15, activation='relu'))\n",
    "model.add(Dense(encoder_length,activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "epochs = 2\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "history2 = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "\n",
    "\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 152996 samples, validate on 17000 samples\n",
      "Epoch 1/2\n",
      "152996/152996 [==============================] - 121s 792us/step - loss: 1.6439 - acc: 0.3838 - val_loss: 1.6164 - val_acc: 0.3934\n",
      "Epoch 2/2\n",
      "152996/152996 [==============================] - 160s 1ms/step - loss: 1.2858 - acc: 0.5106 - val_loss: 1.7810 - val_acc: 0.3685\n",
      "20000/20000 [==============================] - 3s 138us/step\n",
      "Test score: 1.757355754852295\n",
      "Test accuracy: 0.3752\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "\n",
    "from keras.layers import Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "embedding_size = 100\n",
    "\n",
    "model = Sequential()\n",
    "e = Embedding(max_features,embedding_size,input_length=maxlen)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(15, activation='relu'))\n",
    "model.add(Dense(encoder_length,activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "epochs = 2\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "history2 = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "\n",
    "\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
